{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import the libraries we'll need\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "\n",
    "import os\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams.update({'font.size': 24})\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Functions to help extract the CIFAR-10 binary data\n",
    "def unpickle(file):\n",
    "    import cPickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = cPickle.load(fo)\n",
    "    return dict\n",
    "\n",
    "def extract_data(file):\n",
    "    d = unpickle(file)\n",
    "    dim = int(np.sqrt(d['data'].shape[1]/3))\n",
    "    d['data'] = d['data'].reshape((-1,3,dim,dim))\n",
    "    d['data'] = np.transpose(d['data'],axes=(0,2,3,1))\n",
    "    return d['data'],np.array(d['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cPickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-6f60a32fcee1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'data_batch'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-2a08c1cb52d9>\u001b[0m in \u001b[0;36mextract_data\u001b[0;34m(file)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mextract_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-2a08c1cb52d9>\u001b[0m in \u001b[0;36munpickle\u001b[0;34m(file)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Functions to help extract the CIFAR-10 binary data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0munpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mcPickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mdict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcPickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cPickle'"
     ]
    }
   ],
   "source": [
    "#Extract the data from binary files\n",
    "directory = 'cifar-10-batches-py'\n",
    "X = []\n",
    "Y = []\n",
    "for f in os.listdir(directory):\n",
    "    if 'data_batch' in f:\n",
    "        x,y = extract_data(directory+'/'+f)\n",
    "        X.append(x)\n",
    "        Y.append(y)\n",
    "        \n",
    "print ('found {} datasets'.format(len(X)))\n",
    "        \n",
    "X_train = np.concatenate(X[:-1])\n",
    "Y_train = np.concatenate(Y[:-1])\n",
    "\n",
    "X_val = X[-1]\n",
    "Y_val = Y[-1]\n",
    "\n",
    "X_test, Y_test = extract_data(directory+'/test_batch')\n",
    "\n",
    "print (\"X_train.shape = {}, Y_train.shape = {}\".format(\n",
    "    X_train.shape,Y_train.shape))\n",
    "\n",
    "print (\"X_test.shape = {}, Y_test.shape = {}\".format(\n",
    "    X_test.shape,Y_test.shape))\n",
    "\n",
    "classes = unpickle(directory+'/batches.meta')['label_names']\n",
    "print (classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Plot a random image so we can see what we are working with\n",
    "i = np.random.randint(X_train.shape[0])\n",
    "plt.figure()\n",
    "plt.imshow(X_train[i,:,:,:])\n",
    "plt.colorbar()\n",
    "plt.grid('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#normalize the data\n",
    "X_train = 1.0*X_train/255\n",
    "X_val = 1.0*X_val/255\n",
    "X_test = 1.0*X_test/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Plot the same image to make sure the normalization worked\n",
    "plt.figure()\n",
    "plt.imshow(X_train[i,:,:,:])\n",
    "plt.colorbar()\n",
    "plt.grid('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define some Tensorflow convenience functions to help build the neural network\n",
    "def conv2D(x, dims=[3,3], filters=32, strides=[1,1], \n",
    "           std=1e-3, padding='SAME', activation=tf.identity, scope='conv2d', reuse=False):\n",
    "    \n",
    "    with tf.variable_scope(scope,reuse=reuse):\n",
    "        \n",
    "        s = x.get_shape().as_list()\n",
    "        shape = dims +[s[3],filters]\n",
    "        \n",
    "        W = tf.Variable(tf.random_normal(shape=shape,stddev=std),\n",
    "            name='W')\n",
    "        b = tf.Variable(tf.ones([filters])*std, name='b')\n",
    "    \n",
    "        o = tf.nn.convolution(x, W, padding, strides=strides)\n",
    "        \n",
    "        o = o+b\n",
    "        \n",
    "        a = activation(o)\n",
    "        \n",
    "        return a\n",
    "    \n",
    "def fullyConnected(x,output_units=100,activation=tf.identity,std=1e-3,\n",
    "                  scope='fc',reuse=False):\n",
    "    with tf.variable_scope(scope,reuse=reuse):\n",
    "        \n",
    "        s = x.get_shape().as_list()\n",
    "        shape = [s[1],output_units]\n",
    "        \n",
    "        W = tf.Variable(tf.random_normal(shape, stddev=std))\n",
    "        b = tf.Variable(tf.ones([shape[1]])*std)\n",
    "\n",
    "        h = tf.matmul(x,W)+b\n",
    "        a = activation(h)\n",
    "        return a\n",
    "    \n",
    "def get_batch(X,Y,N,n=32):\n",
    "    inds = np.random.choice(range(N),size=n, replace=False)\n",
    "    x = X[inds]\n",
    "    y = Y[inds]\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define neural network and training parameters\n",
    "train_steps = 15000\n",
    "N = X_train.shape[0]\n",
    "Nval = X_val.shape[0]\n",
    "Nbatch = 32\n",
    "print_step = 1000\n",
    "W = X_train.shape[1]\n",
    "H = W\n",
    "C = 3\n",
    "filters = 16\n",
    "dims = [5,5]\n",
    "pool_dims = [2,2]\n",
    "strides = [1,1]\n",
    "hidden_size = 100\n",
    "act = tf.nn.relu\n",
    "std=1e-2\n",
    "num_classes=10\n",
    "learning_rate=5e-3\n",
    "momentum=0.9\n",
    "l2_coeff = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Tensorflow graph construction\n",
    "\n",
    "#construct input place holders\n",
    "x = tf.placeholder(shape=[None,W,H,C],dtype=tf.float32)\n",
    "y = tf.placeholder(shape=[None],dtype=tf.int32)\n",
    "\n",
    "o = conv2D(x,dims=dims,filters=filters,strides=strides,std=std,\n",
    "           activation=act, scope='conv_1')\n",
    "\n",
    "o = tf.nn.pool(o,pool_dims,'MAX',padding='VALID')\n",
    "\n",
    "o = conv2D(o,dims=dims,filters=filters,strides=strides,std=std,\n",
    "           activation=act, scope='conv_3')\n",
    "\n",
    "o = tf.nn.pool(o,pool_dims,'MAX', padding='VALID')\n",
    "\n",
    "s = o.get_shape().as_list()\n",
    "o = tf.reshape(o,[-1,s[1]*s[2]*s[3]])\n",
    "\n",
    "o = fullyConnected(o,hidden_size,scope='dense')\n",
    "\n",
    "o = fullyConnected(o,num_classes,scope='fc_logits')\n",
    "\n",
    "yhat = tf.nn.softmax(o)\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=o,labels=y))\n",
    "\n",
    "#Apply L2 regularization\n",
    "var_list = tf.trainable_variables()\n",
    "\n",
    "l2_reg = 0\n",
    "for v in var_list:\n",
    "    l2_reg += tf.reduce_mean(tf.square(v))\n",
    "    \n",
    "l2_reg = (1.0/len(var_list))*l2_coeff*l2_reg\n",
    "loss = loss + l2_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Construct the optimizer and training operations\n",
    "opt = tf.train.MomentumOptimizer(learning_rate,momentum)\n",
    "train = opt.minimize(loss)\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Construct the tensorflow session and initialize the graph\n",
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Start the train loop\n",
    "train_hist = []\n",
    "val_hist = []\n",
    "\n",
    "for i in range(train_steps):\n",
    "    xb,yb = get_batch(X_train,Y_train,N,n=Nbatch)\n",
    "    l,_=sess.run([loss,train],{x:xb,y:yb})\n",
    "\n",
    "    if i%print_step == 0:\n",
    "        xb,yb = get_batch(X_val,Y_val,Nval,n=Nbatch)\n",
    "        lval=sess.run(loss,{x:xb,y:yb})\n",
    "        print (\"iter: {} Train: {}, Val: {}\".format(i,l,lval))\n",
    "        train_hist.append(l)\n",
    "        val_hist.append(lval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(train_hist, linewidth=2,color='r',label='train')\n",
    "plt.plot(val_hist, linewidth=2,color='g',label='val')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Plot a random digit and the corresponding network output to see\n",
    "#how well our network is working\n",
    "index = np.random.randint(X_test.shape[0])\n",
    "img = X_test[index,:]\n",
    "plt.figure()\n",
    "plt.imshow(img,cmap='gray')\n",
    "plt.colorbar()\n",
    "plt.savefig('probability_image.pdf',dpi=600)\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "#We need a batch dimension so we reshape the vector to (1,32,32,3)\n",
    "input_vector = X_test[index,:]\n",
    "input_vector = input_vector.reshape((1,32,32,3))\n",
    "class_probabilities = sess.run(yhat,{x:input_vector})\n",
    "\n",
    "plt.figure()\n",
    "plt.stem(class_probabilities[0])\n",
    "plt.xlabel('image class')\n",
    "plt.ylabel('probability')\n",
    "plt.xticks(np.arange(10),classes, rotation=90)\n",
    "plt.savefig('probability.pdf',dpi=600)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#calculate the accuracy for each class\n",
    "N_test = X_test.shape[0]\n",
    "predictions = []\n",
    "for i in range(0,N,Nbatch):\n",
    "    xb = X_test[i:i+Nbatch]\n",
    "    predicted_probabilities = sess.run(yhat,{x:xb})\n",
    "    predictions.append(predicted_probabilities)\n",
    "predictions = np.concatenate(predictions)\n",
    "predicted_classes = np.argmax(predictions,axis=1)\n",
    "\n",
    "accuracies = []\n",
    "for i in range(num_classes):\n",
    "    class_indices = np.where(Y_test==i)[0]\n",
    "    total_correct = np.sum(predicted_classes[class_indices]==Y_test[class_indices])\n",
    " \n",
    "    acc = 1.0*total_correct/len(class_indices)\n",
    "    accuracies.append(acc)\n",
    "    print (\"Accuracy for class {} = {}%\".format(classes[i],acc*100))\n",
    "    \n",
    "plt.figure()\n",
    "plt.stem(accuracies)\n",
    "plt.xlabel('image class')\n",
    "plt.ylabel('test accuracy')\n",
    "plt.xticks(np.arange(10),classes, rotation=90)\n",
    "plt.savefig('accuracy.pdf',dpi=600)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
